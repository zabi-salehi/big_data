>This is a project for the lecture "Big Data" in third and fourth semester. Course: WWI20DSB  

Contributors (matr. number):    
- Anh Vu (1039624)   
- Florian Frey (7199749)
- Frederick Neugebauer (4521985)
- Olena Lavrikova (5436924)
- Zabiullah Salehi (7891497)

# Idea: Popular Netflix Movies and Shows

The idea behind the project is to score movies and tv shows according to their views on the website.  
The movie or tv show should get a number of points for each view on itself and for everytime another show or movie with the same director gets a view.  
By that a top list should be created to show the most successfull shows and movies. With that top list users can easily access popular content without having to directly search for it. 

# Data

To implement the data needed to accomplish the described idea the ["Netflix Movies and TV Shows" dataset](https://www.kaggle.com/datasets/shivamb/netflix-shows) from Kaggle was used.
The dataset is also saved as `netflix_titles.csv` in the data directory.  
After some pre-processing the dataset contains the following attributes:
```
title:			Title of the movie or series.
director:		Main director of the title.
cast:			Actors participating in the movie/series.
country:		Country the title was produced in.
release_year:	Release year.
duration:		Film duration or amount of seasons.
genre:			List of genres.
description:	Short summary of the movie/series.
```
As seen in the following sample:
![Screenshot of the Data](src/netflix_data.png)


# Architecture

![Big Data Platform Architecture ](src/big_data_platform.png)

The figure above shows the architecture of a typical big data application.  
On the left the internet, the users who will access the web application are pictured. A load balancer helps to distribute the user requests to the instances of our web servers. The web servers then are connected to the database of the application and several cache servers which help to relieve traffic on the database.  
The three frames on the right are essential for a big data application. The web servers send data generated by the users to the big data messaging service. The service can send the data to the processing unit or store the data stream into a data lake from where it can be processed later on. The computed results are written into the database.

Because of a large amount of data, which should be coordinated and processed, Spark is used in the application. Spark distributes the data into clusters. Furthermore, Spark offers high speed for processing the data and gives good performance to the application. Besides that, Kafka concentrates on the data stream, that are exchanged between data receiver and data source. This open source streaming system optimizes a high error tolerance and scalability in the application. Also it is prevented that the transmitter overloads the receiver. In addition, if the transmission crashes through Apache Kafka, the sender receives feedback about the failure.

- erklÃ¤ren welche implementierungen wir fÃ¼r die einzelnen teile verwendet haben **und warum** 
- ggf. die Komponenten noch mehr im Detail erklÃ¤ren
- ob auf eine Lambda- oder Kappa-Architektur aufgebaut wird

# Implementation

## Content

The repository contains the following directories:  

**ðŸ“‚data**   
âž¡ containing the dataset and a jupyter notebook to preprocess and convert the data into ddl statements, saving the results into `ddl.txt`  

**ðŸ“‚k8s**  
âž¡ .yaml-files used to define the deployment   

**ðŸ“‚spark-app**  
âž¡ spark application which processes the kafka stream messages and saves the results into the database  

**ðŸ“‚src**   
âž¡ contains images and other material used in this documentation  

**ðŸ“‚web-app**  
âž¡ contains necessary elements to build the web application

And the `ðŸ“„skaffold.yaml` file to run the cluster (More on this in [Prerequisites](#Prerequisites)).


## Coding

- erklÃ¤ren von index.js, spark-app.py und transform_data.ipynb

# Final Application Showcase

- Show the cast (video demo) if possible
- otherwise describe how the application should look like ideally. 
	For example: per text and drawn picture ;)


# Prerequisites

To run the application on your local machine you need installations of [Docker Desktop](https://www.docker.com/products/docker-desktop/), [Minikube](https://minikube.sigs.k8s.io/docs/start/), [Helm](https://helm.sh/docs/intro/install/) and [Skaffold](https://skaffold.dev/docs/install/). Please refer to the linked documentations for instructions on how to install the software on your operating system.

Start your Kubernetes Cluster with the Docker Engine by executing
```
minikube start --driver=docker --memory 14336 --cpus 4
```

Deploy a Strimzi.io Kafka operator
```
helm repo add strimzi http://strimzi.io/charts/
helm install my-kafka-operator strimzi/strimzi-kafka-operator
kubectl apply -f https://farberg.de/talks/big-data/code/helm-kafka-operator/kafka-cluster-def.yaml
```

Start a Hadoop cluster with YARN (for checkpointing)
```
minikube ssh docker pull danisla/hadoop:2.9.0
helm repo add stable https://charts.helm.sh/stable
helm install --namespace=default --set hdfs.dataNode.replicas=1 --set yarn.nodeManager.replicas=1 --set hdfs.webhdfs.enabled=true my-hadoop-cluster stable/hadoop
```

To deploy the remaining pods use
```
skaffold run
```

When the database is ready (check this via `kubectl logs \<mariadb-pod-name>`) populate it with data:
```
kubectl exec -i \<mariadb-pod-name> -- mariadb -uroot -pmysecretpw popular < data/mariadb-dump.sql
```

Run the following command and access the app via the url you'll receive:
```
minikube service popular-slides-service --url
```